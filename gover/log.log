2019-03-08 22:47:50 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 22:47:50 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 22:47:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 22:47:50 [scrapy.extensions.telnet] INFO: Telnet Password: f0ef6aa2dbaf8ed4
2019-03-08 22:47:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 22:47:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 22:47:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 22:47:50 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 22:47:50 [scrapy.core.engine] INFO: Spider opened
2019-03-08 22:47:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 22:47:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 22:47:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/top/ruse.shtml> (referer: None)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 36, in parse
    next_url = response.xpath('//a/text()=">"/@href').extract_first()
  File "d:\anaconda3\lib\site-packages\scrapy\http\response\text.py", line 119, in xpath
    return self.selector.xpath(query, **kwargs)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 242, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "d:\anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid type in //a/text()=">"/@href
2019-03-08 22:47:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-03-08 22:47:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14486,
 'downloader/request_count': 33,
 'downloader/request_method_count/GET': 33,
 'downloader/response_bytes': 340930,
 'downloader/response_count': 33,
 'downloader/response_status_count/200': 32,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 14, 47, 52, 801936),
 'item_scraped_count': 30,
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'request_depth_max': 1,
 'response_received_count': 32,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2019, 3, 8, 14, 47, 50, 828592)}
2019-03-08 22:47:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-03-08 22:49:54 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 22:49:54 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 22:49:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 22:49:54 [scrapy.extensions.telnet] INFO: Telnet Password: e097bb3ba0d06efc
2019-03-08 22:49:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 22:49:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 22:49:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 22:49:55 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 22:49:55 [scrapy.core.engine] INFO: Spider opened
2019-03-08 22:49:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 22:49:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 22:50:55 [scrapy.extensions.logstats] INFO: Crawled 77 pages (at 77 pages/min), scraped 30 items (at 30 items/min)
2019-03-08 22:51:55 [scrapy.extensions.logstats] INFO: Crawled 120 pages (at 43 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 22:52:55 [scrapy.extensions.logstats] INFO: Crawled 163 pages (at 43 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 22:53:55 [scrapy.extensions.logstats] INFO: Crawled 207 pages (at 44 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 22:54:55 [scrapy.extensions.logstats] INFO: Crawled 252 pages (at 45 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 22:55:55 [scrapy.extensions.logstats] INFO: Crawled 294 pages (at 42 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 22:56:55 [scrapy.extensions.logstats] INFO: Crawled 335 pages (at 41 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 22:57:55 [scrapy.extensions.logstats] INFO: Crawled 376 pages (at 41 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 22:58:54 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 22:58:54 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 22:58:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 22:58:54 [scrapy.extensions.telnet] INFO: Telnet Password: 5de295e1ae320651
2019-03-08 22:58:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 22:58:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 22:58:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 22:58:55 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 22:58:55 [scrapy.core.engine] INFO: Spider opened
2019-03-08 22:58:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 22:58:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 22:59:55 [scrapy.extensions.logstats] INFO: Crawled 174 pages (at 174 pages/min), scraped 30 items (at 30 items/min)
2019-03-08 23:00:40 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 23:00:40 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 23:00:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 23:00:40 [scrapy.extensions.telnet] INFO: Telnet Password: b0103cbee9d8a45e
2019-03-08 23:00:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 23:00:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 23:00:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 23:00:40 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 23:00:40 [scrapy.core.engine] INFO: Spider opened
2019-03-08 23:00:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 23:00:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 23:01:40 [scrapy.extensions.logstats] INFO: Crawled 213 pages (at 213 pages/min), scraped 30 items (at 30 items/min)
2019-03-08 23:02:40 [scrapy.extensions.logstats] INFO: Crawled 407 pages (at 194 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:03:23 [scrapy.core.engine] INFO: Closing spider (finished)
2019-03-08 23:03:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 234111,
 'downloader/request_count': 442,
 'downloader/request_method_count/GET': 442,
 'downloader/response_bytes': 12337555,
 'downloader/response_count': 442,
 'downloader/response_status_count/200': 441,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 15, 3, 23, 580188),
 'item_scraped_count': 30,
 'log_count/INFO': 11,
 'request_depth_max': 409,
 'response_received_count': 441,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 440,
 'scheduler/dequeued/memory': 440,
 'scheduler/enqueued': 440,
 'scheduler/enqueued/memory': 440,
 'start_time': datetime.datetime(2019, 3, 8, 15, 0, 40, 569023)}
2019-03-08 23:03:23 [scrapy.core.engine] INFO: Spider closed (finished)
2019-03-08 23:28:25 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 23:28:25 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 23:28:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 23:28:25 [scrapy.extensions.telnet] INFO: Telnet Password: 3bc5002b801ae4a9
2019-03-08 23:28:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 23:28:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 23:28:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 23:28:25 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 23:28:25 [scrapy.core.engine] INFO: Spider opened
2019-03-08 23:28:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 23:28:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 23:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403848.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid predicate

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 47, in parse_detail
    items['detail_content'] = jud.xpath('../div[@class=""contentext]/text()').extract()
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in xpath
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in <listcomp>
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 242, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "d:\anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid predicate in ../div[@class=""contentext]/text()
2019-03-08 23:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403628.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid predicate

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 47, in parse_detail
    items['detail_content'] = jud.xpath('../div[@class=""contentext]/text()').extract()
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in xpath
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in <listcomp>
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 242, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "d:\anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid predicate in ../div[@class=""contentext]/text()
2019-03-08 23:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403236.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid predicate

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 47, in parse_detail
    items['detail_content'] = jud.xpath('../div[@class=""contentext]/text()').extract()
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in xpath
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in <listcomp>
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 242, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "d:\anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid predicate in ../div[@class=""contentext]/text()
2019-03-08 23:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403292.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid predicate

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 47, in parse_detail
    items['detail_content'] = jud.xpath('../div[@class=""contentext]/text()').extract()
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in xpath
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in <listcomp>
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 242, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "d:\anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid predicate in ../div[@class=""contentext]/text()
2019-03-08 23:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403732.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid predicate

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 47, in parse_detail
    items['detail_content'] = jud.xpath('../div[@class=""contentext]/text()').extract()
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in xpath
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in <listcomp>
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 242, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "d:\anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid predicate in ../div[@class=""contentext]/text()
2019-03-08 23:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403202.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
lxml.etree.XPathEvalError: Invalid predicate

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 47, in parse_detail
    items['detail_content'] = jud.xpath('../div[@class=""contentext]/text()').extract()
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in xpath
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 84, in <listcomp>
    return self.__class__(flatten([x.xpath(xpath, namespaces=namespaces, **kwargs) for x in self]))
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 242, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "d:\anaconda3\lib\site-packages\six.py", line 692, in reraise
    raise value.with_traceback(tb)
  File "d:\anaconda3\lib\site-packages\parsel\selector.py", line 238, in xpath
    **kwargs)
  File "src\lxml\etree.pyx", line 1586, in lxml.etree._Element.xpath
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result
ValueError: XPath error: Invalid predicate in ../div[@class=""contentext]/text()
2019-03-08 23:29:42 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 23:29:42 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 23:29:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 23:29:42 [scrapy.extensions.telnet] INFO: Telnet Password: 606d7708c71fb12d
2019-03-08 23:29:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 23:29:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 23:29:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 23:29:42 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 23:29:42 [scrapy.core.engine] INFO: Spider opened
2019-03-08 23:29:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 23:29:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 23:30:42 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 109 pages/min), scraped 30 items (at 30 items/min)
2019-03-08 23:31:42 [scrapy.extensions.logstats] INFO: Crawled 157 pages (at 48 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:32:42 [scrapy.extensions.logstats] INFO: Crawled 202 pages (at 45 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:33:42 [scrapy.extensions.logstats] INFO: Crawled 248 pages (at 46 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:34:42 [scrapy.extensions.logstats] INFO: Crawled 293 pages (at 45 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:35:42 [scrapy.extensions.logstats] INFO: Crawled 338 pages (at 45 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:36:49 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 23:36:49 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 23:36:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 23:36:49 [scrapy.extensions.telnet] INFO: Telnet Password: 30602463b27ee288
2019-03-08 23:36:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 23:36:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 23:36:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 23:36:50 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 23:36:50 [scrapy.core.engine] INFO: Spider opened
2019-03-08 23:36:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 23:36:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 23:37:50 [scrapy.extensions.logstats] INFO: Crawled 214 pages (at 214 pages/min), scraped 30 items (at 30 items/min)
2019-03-08 23:38:50 [scrapy.extensions.logstats] INFO: Crawled 387 pages (at 173 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:39:22 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 23:39:22 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 23:39:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 23:39:22 [scrapy.extensions.telnet] INFO: Telnet Password: 5871952001f5208d
2019-03-08 23:39:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 23:39:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 23:39:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 23:39:22 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 23:39:22 [scrapy.core.engine] INFO: Spider opened
2019-03-08 23:39:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 23:39:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 23:40:35 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 23:40:35 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 23:40:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 23:40:35 [scrapy.extensions.telnet] INFO: Telnet Password: e3525e8c749b7a45
2019-03-08 23:40:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 23:40:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 23:40:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 23:40:35 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 23:40:35 [scrapy.core.engine] INFO: Spider opened
2019-03-08 23:40:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 23:40:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-08 23:41:35 [scrapy.extensions.logstats] INFO: Crawled 255 pages (at 255 pages/min), scraped 30 items (at 30 items/min)
2019-03-08 23:42:35 [scrapy.extensions.logstats] INFO: Crawled 425 pages (at 170 pages/min), scraped 30 items (at 0 items/min)
2019-03-08 23:42:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-03-08 23:42:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 234111,
 'downloader/request_count': 442,
 'downloader/request_method_count/GET': 442,
 'downloader/response_bytes': 12344239,
 'downloader/response_count': 442,
 'downloader/response_status_count/200': 441,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 15, 42, 56, 544831),
 'item_scraped_count': 30,
 'log_count/INFO': 11,
 'request_depth_max': 409,
 'response_received_count': 441,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 440,
 'scheduler/dequeued/memory': 440,
 'scheduler/enqueued': 440,
 'scheduler/enqueued/memory': 440,
 'start_time': datetime.datetime(2019, 3, 8, 15, 40, 35, 349627)}
2019-03-08 23:42:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-03-08 23:50:32 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-08 23:50:32 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-08 23:50:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-08 23:50:32 [scrapy.extensions.telnet] INFO: Telnet Password: fbf6aa656f538ad7
2019-03-08 23:50:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-08 23:50:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-08 23:50:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-08 23:50:32 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline']
2019-03-08 23:50:32 [scrapy.core.engine] INFO: Spider opened
2019-03-08 23:50:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-08 23:50:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:02:28 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-09 00:02:28 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-09 00:02:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-09 00:02:28 [scrapy.extensions.telnet] INFO: Telnet Password: b6f1133075413077
2019-03-09 00:02:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-09 00:02:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-09 00:02:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-09 00:02:28 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline', 'gover.pipelines.MongodbPipeline']
2019-03-09 00:02:28 [scrapy.core.engine] INFO: Spider opened
2019-03-09 00:02:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:02:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:03:02 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-08 15:53:11',
 'detail_content': [''],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403856.shtml',
 'location': '',
 'num': '209488',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: mongo_uri:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:03:32 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-06 10:25:08',
 'detail_content': ['(),:',
                    '1.21(2),3,',
                    '',
                    '!!!',
                    ':,',
                    '2.,,!',
                    ':,,'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403638.shtml',
 'location': '',
 'num': '209317',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: mongo_uri:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:04:02 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-05 10:52:28',
 'detail_content': ['NN',
                    '',
                    '',
                    '',
                    ',0769-81695566',
                    '',
                    '',
                    '',
                    '20193'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403567.shtml',
 'location': '',
 'num': '209266',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: mongo_uri:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:04:33 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-07 16:14:12',
 'detail_content': ['848--~~1.5'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403784.shtml',
 'location': '',
 'num': '209433',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: mongo_uri:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:05:03 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-07 14:24:08',
 'detail_content': [''],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403729.shtml',
 'location': '',
 'num': '209389',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: mongo_uri:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:07:51 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-09 00:07:51 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-09 00:07:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-09 00:07:51 [scrapy.extensions.telnet] INFO: Telnet Password: 5bef46dab48513f9
2019-03-09 00:07:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-09 00:07:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-09 00:07:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-09 00:07:51 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline', 'gover.pipelines.MongodbPipeline']
2019-03-09 00:07:51 [scrapy.core.engine] INFO: Spider opened
2019-03-09 00:07:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:07:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403525.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403531.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403550.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403567.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403628.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403537.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403624.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403559.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403638.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403729.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403784.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403732.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403848.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403856.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403202.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403233.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403798.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403248.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403236.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403372.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403292.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403373.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403326.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403332.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403376.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403401.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403452.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403461.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403471.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403523.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:33 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-09 00:09:33 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-09 00:09:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-09 00:09:33 [scrapy.extensions.telnet] INFO: Telnet Password: 14673596bc62edc8
2019-03-09 00:09:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-09 00:09:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-09 00:09:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-09 00:09:33 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline', 'gover.pipelines.MongodbPipeline']
2019-03-09 00:09:33 [scrapy.core.engine] INFO: Spider opened
2019-03-09 00:09:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:09:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403523.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403567.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403525.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403550.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403531.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403624.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403559.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403537.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403628.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403638.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403732.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403784.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403729.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403798.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403848.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403856.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403202.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403233.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403248.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403236.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403292.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403332.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403326.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403372.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403373.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403376.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403401.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403452.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403461.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:09:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wz.sun0769.com/html/question/201903/403471.shtml> (referer: http://wz.sun0769.com/html/top/ruse.shtml)
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\ScrapyProject\gover\gover\spiders\ygzw.py", line 38, in parse_detail
    item = response.meta["item"]
KeyError: 'item'
2019-03-09 00:10:33 [scrapy.extensions.logstats] INFO: Crawled 90 pages (at 90 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:11:33 [scrapy.extensions.logstats] INFO: Crawled 135 pages (at 45 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:12:19 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-09 00:12:19 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-09 00:12:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-09 00:12:19 [scrapy.extensions.telnet] INFO: Telnet Password: a3ed9ca6566ca6d5
2019-03-09 00:12:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-09 00:12:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-09 00:12:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-09 00:12:20 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline', 'gover.pipelines.MongodbPipeline']
2019-03-09 00:12:20 [scrapy.core.engine] INFO: Spider opened
2019-03-09 00:12:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:12:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:12:53 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-08 15:39:36',
 'detail_content': [''],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403848.shtml',
 'location': '',
 'num': '209482',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:13:23 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-08 15:53:11',
 'detail_content': [''],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403856.shtml',
 'location': '',
 'num': '209488',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:13:53 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-07 14:29:21',
 'detail_content': ['5'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403732.shtml',
 'location': '',
 'num': '209392',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:14:23 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-05 10:52:28',
 'detail_content': ['NN',
                    '',
                    '',
                    '',
                    ',0769-81695566',
                    '',
                    '',
                    '',
                    '20193'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403567.shtml',
 'location': '',
 'num': '209266',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:20:07 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-09 00:20:07 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-09 00:20:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-09 00:20:07 [scrapy.extensions.telnet] INFO: Telnet Password: 30c42fa3229fca56
2019-03-09 00:20:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-09 00:20:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-09 00:20:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-09 00:20:07 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline', 'gover.pipelines.MongodbPipeline']
2019-03-09 00:20:07 [scrapy.core.engine] INFO: Spider opened
2019-03-09 00:20:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:20:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:20:40 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-04 16:50:54',
 'detail_content': ['70/9012050'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403523.shtml',
 'location': '',
 'num': '209230',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:21:11 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-05 16:47:55',
 'detail_content': ['',
                    '',
                    '',
                    '',
                    '',
                    '1',
                    '2',
                    '',
                    '3',
                    '4',
                    '5',
                    '6',
                    '7',
                    '8',
                    '',
                    '9',
                    '',
                    '',
                    '',
                    '10',
                    '',
                    '11',
                    'XX',
                    '',
                    ''],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403624.shtml',
 'location': '',
 'num': '209302',
 'state': '',
 'theme': ' '}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:21:41 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-05 10:52:28',
 'detail_content': ['NN',
                    '',
                    '',
                    '',
                    ',0769-81695566',
                    '',
                    '',
                    '',
                    '20193'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403567.shtml',
 'location': '',
 'num': '209266',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:29:41 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-09 00:29:41 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-09 00:29:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-09 00:29:41 [scrapy.extensions.telnet] INFO: Telnet Password: 4ba22d06d530b9ad
2019-03-09 00:29:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-09 00:29:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-09 00:29:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-09 00:29:42 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline', 'gover.pipelines.MongodbPipeline']
2019-03-09 00:29:42 [scrapy.core.engine] INFO: Spider opened
2019-03-09 00:29:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:29:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:30:15 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-08 15:53:11',
 'detail_content': [''],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403856.shtml',
 'location': '',
 'num': '209488',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:30:45 [scrapy.core.scraper] ERROR: Error processing {'date': '2019-03-04 15:32:03',
 'detail_content': ['...'],
 'detail_img': [],
 'detail_url': 'http://wz.sun0769.com/html/question/201903/403471.shtml',
 'location': '',
 'num': '209195',
 'state': '',
 'theme': ''}
Traceback (most recent call last):
  File "d:\anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "E:\ScrapyProject\gover\gover\pipelines.py", line 39, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 3161, in insert
    check_keys, manipulate, write_concern)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 607, in _insert
    bypass_doc_val, session)
  File "d:\anaconda3\lib\site-packages\pymongo\collection.py", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File "d:\anaconda3\lib\contextlib.py", line 112, in __enter__
    return next(self.gen)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1516, in __start_session
    server_session = self._get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\mongo_client.py", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 427, in get_server_session
    None)
  File "d:\anaconda3\lib\site-packages\pymongo\topology.py", line 199, in _select_servers_loop
    self._error_message(selector))
pymongo.errors.ServerSelectionTimeoutError: lcoalhost:27017: [Errno 11001] getaddrinfo failed
2019-03-09 00:31:23 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: gover)
2019-03-09 00:31:23 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 17.2.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0
2019-03-09 00:31:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'gover', 'LOG_FILE': './log.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'gover.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['gover.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
2019-03-09 00:31:23 [scrapy.extensions.telnet] INFO: Telnet Password: 6c4616e6f412a499
2019-03-09 00:31:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-03-09 00:31:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-03-09 00:31:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-03-09 00:31:23 [scrapy.middleware] INFO: Enabled item pipelines:
['gover.pipelines.GoverPipeline', 'gover.pipelines.MongodbPipeline']
2019-03-09 00:31:23 [scrapy.core.engine] INFO: Spider opened
2019-03-09 00:31:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-09 00:31:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-03-09 00:32:23 [scrapy.extensions.logstats] INFO: Crawled 162 pages (at 162 pages/min), scraped 30 items (at 30 items/min)
2019-03-09 00:33:23 [scrapy.extensions.logstats] INFO: Crawled 203 pages (at 41 pages/min), scraped 30 items (at 0 items/min)
2019-03-09 00:34:23 [scrapy.extensions.logstats] INFO: Crawled 246 pages (at 43 pages/min), scraped 30 items (at 0 items/min)
2019-03-09 00:35:23 [scrapy.extensions.logstats] INFO: Crawled 287 pages (at 41 pages/min), scraped 30 items (at 0 items/min)
2019-03-09 00:36:23 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 39 pages/min), scraped 30 items (at 0 items/min)
2019-03-09 00:37:23 [scrapy.extensions.logstats] INFO: Crawled 366 pages (at 40 pages/min), scraped 30 items (at 0 items/min)
2019-03-09 00:38:23 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 40 pages/min), scraped 30 items (at 0 items/min)
2019-03-09 00:39:14 [scrapy.core.engine] INFO: Closing spider (finished)
2019-03-09 00:39:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 234184,
 'downloader/request_count': 442,
 'downloader/request_method_count/GET': 442,
 'downloader/response_bytes': 12337460,
 'downloader/response_count': 442,
 'downloader/response_status_count/200': 441,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 16, 39, 14, 866993),
 'item_scraped_count': 30,
 'log_count/INFO': 16,
 'request_depth_max': 409,
 'response_received_count': 441,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 440,
 'scheduler/dequeued/memory': 440,
 'scheduler/enqueued': 440,
 'scheduler/enqueued/memory': 440,
 'start_time': datetime.datetime(2019, 3, 8, 16, 31, 23, 816674)}
2019-03-09 00:39:14 [scrapy.core.engine] INFO: Spider closed (finished)
